@misc{pygetscope,
author = {Zendrikov, Dmitrii},
title = {PyGetScope python wrapper for Agilent oscilloscopes},
year = {2020}, 
howpublished = "\url{https://code.ini.uzh.ch/ncs/libs/pygetscope}"}


@misc{pyvisa,
title = {PyVISA: Control your instruments with Python},
howpublished = "\url{https://pyvisa.readthedocs.io/en/latest/}"}


@article{Cariani_etal22,
  title={Time Is of the Essence: Neural Codes, Synchronies, Oscillations, Architectures},
  author={Peter A. Cariani and Janet M. Baker},
  journal={Frontiers in Computational Neuroscience},
  year={2022},
  volume={16},
  url={https://api.semanticscholar.org/CorpusID:249717610}
}

@Inbook{Colinge2002,
title="The MOS Transistor",
bookTitle="Physics of Semiconductor Devices",
year="2002",
publisher="Springer US",
address="Boston, MA",
pages="165--250",
isbn="978-0-306-47622-8",
doi="10.1007/0-306-47622-3_7",
url="https://doi.org/10.1007/0-306-47622-3_7"
}

@article{Riehle_etal97,
author = {Alexa Riehle  and Sonja Grün  and Markus Diesmann  and Ad Aertsen },
title = {Spike Synchronization and Rate Modulation Differentially Involved in Motor Cortical Function},
journal = {Science},
volume = {278},
number = {5345},
pages = {1950-1953},
year = {1997},
doi = {10.1126/science.278.5345.1950},
URL = {https://www.science.org/doi/abs/10.1126/science.278.5345.1950},
eprint = {https://www.science.org/doi/pdf/10.1126/science.278.5345.1950},
abstract = {It is now commonly accepted that planning and execution of movements are based on distributed processing by neuronal populations in motor cortical areas. It is less clear, though, how these populations organize dynamically to cope with the momentary computational demands. Simultaneously recorded activities of neurons in the primary motor cortex of monkeys during performance of a delayed-pointing task exhibited context-dependent, rapid changes in the patterns of coincident action potentials. Accurate spike synchronization occurred in relation to external events (stimuli, movements) and was commonly accompanied by discharge rate modulations but without precise time locking of the spikes to these external events. Spike synchronization also occurred in relation to purely internal events (stimulus expectancy), where firing rate modulations were distinctly absent. These findings indicate that internally generated synchronization of individual spike discharges may subserve the cortical organization of cognitive motor processes.}}

@Article{Moradi04,
author={Moradi, Farshad},
title={Information coding and oscillatory activity in synfire neural networks with and without inhibitory coupling},
journal={Biological Cybernetics},
year={2004},
month={Oct},
day={01},
volume={91},
number={5},
pages={283-294},
abstract={When a population spike (pulse-packet) propagates through a feedforward network with random excitatory connections, it either evolves to a sustained stable level of synchronous activity or fades away (Diesmann et al. in Nature 402:529--533 1999; Cateau and Fukai Neur Netw 14:675--685 2001). Here I demonstrate that in the presence of noise, the probability of the survival of the pulse-packet (or, equivalently, the firing rate of output neurons) reflects the intensity of the input. Furthermore, inhibitory coupling between layers can result in quasi- periodic alternation between several levels of firing activity. These results are obtained by analyzing the evolution of pulse-packet activity as a Markov chain. For the Markov chain analysis, the output of the chain is a linear mapping of the input into a lower-dimensional space, and the eigenvalues and eigenvectors of the transition matrix determine the dynamics of the evolution. Synchronous propagation of firing activity in successive pools of neurons are simulated in networks of integrate-and-fire and compartmental model neurons, and, consistent with the discrete Markov process, the activation of each pool is observed to be predominantly dependent upon the number of cells that fired in the previous pool. Simulation results agree with the numerical solutions of the Markov model. When inhibitory coupling between layers are included in the Markov model, some eigenvalues become complex numbers, implying oscillatory dynamics. The quasiperiodic dynamics is validated with simulation with leaky integrate-and-fire neurons. The networks demonstrate different modes of quasiperiodic activity as the inhibition or excitation parameters of the network are varied.},
issn={1432-0770},
doi={10.1007/s00422-004-0499-x},
url={https://doi.org/10.1007/s00422-004-0499-x}
}

@inproceedings{Shahsavari_etal21,
author = {Shahsavari, Mahyar and Thomas, David and Brown, Andrew and Luk, Wayne},
title = {Neuromorphic Design Using Reward-based STDP Learning on Event-Based Reconfigurable Cluster Architecture},
year = {2021},
isbn = {9781450386913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477145.3477151},
doi = {10.1145/3477145.3477151},
abstract = {Neuromorphic computing systems simulate spiking neural networks that are used for research into how biological neural networks function, as well as for applied engineering such as robotics, pattern recognition, and machine learning. In this paper, we present a neuromorphic system based on an asynchronous event-based hardware platform. We represent three algorithms for implementing spiking networks on our asynchronous hardware platform. We also discuss different trade-offs between synchronisation and messaging costs. A reinforcement learning method known as Reward-modulated STDP is presented as an online learning algorithm in the network. We evaluate the system performance in a single box of our designed architecture using 6000 concurrent hardware threads and demonstrate scaling to networks with up to 2 million neurons and 400 million synapses. The performance of our architecture is also compared to existing neuromorphic platforms, showing a 20 times speed-up over the Brian simulator on an x86 machine, and a 16 times speed-up over a 48-chip SpiNNaker node.},
booktitle = {International Conference on Neuromorphic Systems 2021},
articleno = {5},
numpages = {8},
keywords = {Spiking neural network simulation, Reinforcement Reward-modulated STDP, Reconfigurable architecture., Neuromorphic system},
location = {Knoxville, TN, USA},
series = {ICONS 2021}
}


@ARTICLE{Mozafari_etal18,
  author={Mozafari, Milad and Kheradpisheh, Saeed Reza and Masquelier, Timothée and Nowzari-Dalini, Abbas and Ganjtabesh, Mohammad},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={First-Spike-Based Visual Categorization Using Reward-Modulated STDP}, 
  year={2018},
  volume={29},
  number={12},
  pages={6178-6190},
  keywords={Reinforcement learning;Feature extraction;Image edge detection;Object recognition;Neural networks;Neurons;First-spike-based categorization;reinforcement learning (RL);reward-modulated spike-timing-dependent plasticity (R-STDP);spiking neural networks (SNNs);temporal coding;visual object recognition},
  doi={10.1109/TNNLS.2018.2826721}
}

@article{Cakal_etal24,
doi = {10.1088/2634-4386/ad2ec3},
url = {https://dx.doi.org/10.1088/2634-4386/ad2ec3},
year = {2024},
month = {mar},
publisher = {IOP Publishing},
volume = {4},
number = {1},
pages = {014011},
author = {Ugurcan Cakal and  Maryada and Chenxi Wu and Ilkay Ulusoy and Dylan Richard Muir},
title = {Gradient-descent hardware-aware training and deployment for mixed-signal neuromorphic processors},
journal = {Neuromorphic Computing and Engineering },
abstract = {Mixed-signal neuromorphic processors provide extremely low-power operation for edge inference workloads, taking advantage of sparse asynchronous computation within spiking neural networks (SNNs). However, deploying robust applications to these devices is complicated by limited controllability over analog hardware parameters, as well as unintended parameter and dynamical variations of analog circuits due to fabrication non-idealities. Here we demonstrate a novel methodology for offline training and deployment of SNNs to the mixed-signal neuromorphic processor DYNAP-SE2. Our methodology applies gradient-based training to a differentiable simulation of the mixed-signal device, coupled with an unsupervised weight quantization method to optimize the network’s parameters. Parameter noise injection during training provides robustness to the effects of quantization and device mismatch, making the method a promising candidate for real-world applications under hardware constraints and non-idealities. This work extends Rockpool, an open-source deep-learning library for SNNs, with support for accurate simulation of mixed-signal SNN dynamics. Our approach simplifies the development and deployment process for the neuromorphic community, making mixed-signal neuromorphic processors more accessible to researchers and developers.}
}

@Article{Beuchel_etal21,
author={B{\"u}chel, Julian
and Zendrikov, Dmitrii
and Solinas, Sergio
and Indiveri, Giacomo
and Muir, Dylan R.},
title={Supervised training of spiking neural networks for robust deployment on mixed-signal neuromorphic processors},
journal={Scientific Reports},
year={2021},
month={Dec},
day={03},
volume={11},
number={1},
pages={23376},
abstract={Mixed-signal analog/digital circuits emulate spiking neurons and synapses with extremely high energy efficiency, an approach known as ``neuromorphic engineering''. However, analog circuits are sensitive to process-induced variation among transistors in a chip (``device mismatch''). For neuromorphic implementation of Spiking Neural Networks (SNNs), mismatch causes parameter variation between identically-configured neurons and synapses. Each chip exhibits a different distribution of neural parameters, causing deployed networks to respond differently between chips. Current solutions to mitigate mismatch based on per-chip calibration or on-chip learning entail increased design complexity, area and cost, making deployment of neuromorphic devices expensive and difficult. Here we present a supervised learning approach that produces SNNs with high robustness to mismatch and other common sources of noise. Our method trains SNNs to perform temporal classification tasks by mimicking a pre-trained dynamical system, using a local learning rule from non-linear control theory. We demonstrate our method on two tasks requiring temporal memory, and measure the robustness of our approach to several forms of noise and mismatch. We show that our approach is more robust than common alternatives for training SNNs. Our method provides robust deployment of pre-trained networks on mixed-signal neuromorphic hardware, without requiring per-device training or calibration.},
issn={2045-2322},
doi={10.1038/s41598-021-02779-x},
url={https://doi.org/10.1038/s41598-021-02779-x}
}

@Article{Mariño_etal05,
author={Mari{\~{n}}o, Jorge
and Schummers, James
and Lyon, David C.
and Schwabe, Lars
and Beck, Oliver
and Wiesing, Peter
and Obermayer, Klaus
and Sur, Mriganka},
title={Invariant computations in local cortical networks with balanced excitation and inhibition},
journal={Nature Neuroscience},
year={2005},
month={Feb},
day={01},
volume={8},
number={2},
pages={194-201},
abstract={Cortical computations critically involve local neuronal circuits. The computations are often invariant across a cortical area yet are carried out by networks that can vary widely within an area according to its functional architecture. Here we demonstrate a mechanism by which orientation selectivity is computed invariantly in cat primary visual cortex across an orientation preference map that provides a wide diversity of local circuits. Visually evoked excitatory and inhibitory synaptic conductances are balanced exquisitely in cortical neurons and thus keep the spike response sharply tuned at all map locations. This functional balance derives from spatially isotropic local connectivity of both excitatory and inhibitory cells. Modeling results demonstrate that such covariation is a signature of recurrent rather than purely feed-forward processing and that the observed isotropic local circuit is sufficient to generate invariant spike tuning.},
issn={1546-1726},
doi={10.1038/nn1391},
url={https://doi.org/10.1038/nn1391}
}



@article{WatabeUchida_etal17,
   author = {Watabe-Uchida, Mitsuko and Eshel, Neir and Uchida, Naoshige},
   title = {Neural Circuitry of Reward Prediction Error}, 
   journal= {Annual Review of Neuroscience},
   year = {2017},
   volume = {40},
   pages = {373-394},
   doi = {https://doi.org/10.1146/annurev-neuro-072116-031109},
   url = {https://www.annualreviews.org/content/journals/10.1146/annurev-neuro-072116-031109},
   publisher = {Annual Reviews},
   issn = {1545-4126},
   type = {Journal Article},
   abstract = {Dopamine neurons facilitate learning by calculating reward prediction error, or the difference between expected and actual reward. Despite two decades of research, it remains unclear how dopamine neurons make this calculation. Here we review studies that tackle this problem from a diverse set of approaches, from anatomy to electrophysiology to computational modeling and behavior. Several patterns emerge from this synthesis: that dopamine neurons themselves calculate reward prediction error, rather than inherit it passively from upstream regions; that they combine multiple separate and redundant inputs, which are themselves interconnected in a dense recurrent network; and that despite the complexity of inputs, the output from dopamine neurons is remarkably homogeneous and robust. The more we study this simple arithmetic computation, the knottier it appears to be, suggesting a daunting (but stimulating) path ahead for neuroscience more generally.}
  }


@InProceedings{Gupta_etal15,
  title = 	 {Deep Learning with Limited Numerical Precision},
  author = 	 {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1737--1746},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/gupta15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/gupta15.html},
  abstract = 	 {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network’s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding}
}


@phdthesis{George18,
    author = {Richard Miru George},
    title = {Structural plasticity in neuromorphic systems},
    school = {University of Zurich} ,
    year = {2018}
}


@Article{Reddy_etal2018,
author={Reddy, Gautam
and Wong-Ng, Jerome
and Celani, Antonio
and Sejnowski, Terrence J.
and Vergassola, Massimo},
title={Glider soaring via reinforcement learning in the field},
journal={Nature},
year={2018},
month={Oct},
day={01},
volume={562},
number={7726},
pages={236-239},
abstract={Soaring birds often rely on ascending thermal plumes (thermals) in the atmosphere as they search for prey or migrate across large distances1--4. The landscape of convective currents is rugged and shifts on timescales of a few minutes as thermals constantly form, disintegrate or are transported away by the wind5,6. How soaring birds find and navigate thermals within this complex landscape is unknown. Reinforcement learning7 provides an appropriate framework in which to identify an effective navigational strategy as a sequence of decisions made in response to environmental cues. Here we use reinforcement learning to train a glider in the field to navigate atmospheric thermals autonomously. We equipped a glider of two-metre wingspan with a flight controller that precisely controlled the bank angle and pitch, modulating these at intervals with the aim of gaining as much lift as possible. A navigational strategy was determined solely from the glider's pooled experiences, collected over several days in the field. The strategy relies on on-board methods to accurately estimate the local vertical wind accelerations and the roll-wise torques on the glider, which serve as navigational cues. We establish the validity of our learned flight policy through field experiments, numerical simulations and estimates of the noise in measurements caused by atmospheric turbulence. Our results highlight the role of vertical wind accelerations and roll-wise torques as effective mechanosensory cues for soaring birds and provide a navigational strategy that is directly applicable to the development of autonomous soaring vehicles.},
issn={1476-4687},
doi={10.1038/s41586-018-0533-0},
url={https://doi.org/10.1038/s41586-018-0533-0}
}


@article{Reddy_etal16,
author = {Gautam Reddy  and Antonio Celani  and Terrence J. Sejnowski  and Massimo Vergassola },
title = {Learning to soar in turbulent environments},
journal = {Proceedings of the National Academy of Sciences},
volume = {113},
number = {33},
pages = {E4877-E4884},
year = {2016},
doi = {10.1073/pnas.1606075113},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1606075113},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1606075113},
abstract = {Birds and gliders exploit warm, rising atmospheric currents (thermals) to reach heights comparable to low-lying clouds with a reduced expenditure of energy. This strategy of flight (thermal soaring) is frequently used by migratory birds. Soaring provides a remarkable instance of complex decision making in biology and requires a long-term strategy to effectively use the ascending thermals. Furthermore, the problem is technologically relevant to extend the flying range of autonomous gliders. Thermal soaring is commonly observed in the atmospheric convective boundary layer on warm, sunny days. The formation of thermals unavoidably generates strong turbulent fluctuations, which constitute an essential element of soaring. Here, we approach soaring flight as a problem of learning to navigate complex, highly fluctuating turbulent environments. We simulate the atmospheric boundary layer by numerical models of turbulent convective flow and combine them with model-free, experience-based, reinforcement learning algorithms to train the gliders. For the learned policies in the regimes of moderate and strong turbulence levels, the glider adopts an increasingly conservative policy as turbulence levels increase, quantifying the degree of risk affordable in turbulent environments. Reinforcement learning uncovers those sensorimotor cues that permit effective control over soaring in turbulent environments.}}

@article{Sanda_etal17,
    doi = {10.1371/journal.pcbi.1005705},
    author = {Sanda, Pavel AND Skorheim, Steven AND Bazhenov, Maxim},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Multi-layer network utilizing rewarded spike time dependent plasticity to learn a foraging task},
    year = {2017},
    month = {09},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pcbi.1005705},
    pages = {1-25},
    abstract = {Neural networks with a single plastic layer employing reward modulated spike time dependent plasticity (STDP) are capable of learning simple foraging tasks. Here we demonstrate advanced pattern discrimination and continuous learning in a network of spiking neurons with multiple plastic layers. The network utilized both reward modulated and non-reward modulated STDP and implemented multiple mechanisms for homeostatic regulation of synaptic efficacy, including heterosynaptic plasticity, gain control, output balancing, activity normalization of rewarded STDP and hard limits on synaptic strength. We found that addition of a hidden layer of neurons employing non-rewarded STDP created neurons that responded to the specific combinations of inputs and thus performed basic classification of the input patterns. When combined with a following layer of neurons implementing rewarded STDP, the network was able to learn, despite the absence of labeled training data, discrimination between rewarding patterns and the patterns designated as punishing. Synaptic noise allowed for trial-and-error learning that helped to identify the goal-oriented strategies which were effective in task solving. The study predicts a critical set of properties of the spiking neuronal network with STDP that was sufficient to solve a complex foraging task involving pattern classification and decision making.},
    number = {9},

}